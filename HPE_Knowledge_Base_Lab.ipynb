{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "56cec719",
      "metadata": {},
      "source": [
        "# HPE AI Essentials Knowledge Base Lab\n",
        "\n",
        "## 1. Introduction to Knowledge Base\n",
        "\n",
        "**Knowledge Base** describes a system for building, deploying, and managing secure Retrieval-Augmented Generation (RAG) solutions with GPU-Optimized LLMs in HPE AI Essentials Software.\n",
        "\n",
        "Knowledge Base is an integrated framework that enables you to quickly build and deploy Retrieval-Augmented Generation (RAG) solutions. A RAG solution retrieves relevant information from data sources and uses a generative model to produce accurate, textual responses to user queries.\n",
        "\n",
        "It integrates **retrieval**, **embeddings**, and **generation** processes to reduce development time for Generative AI applications:\n",
        "\n",
        "- **Retrieval**: Finding and accessing relevant information from the documents stored in the vector database.\n",
        "- **Embeddings**: Numerically representing text data, capturing the meaning and context of words and sentences in a machine-understandable format.\n",
        "- **Generation**: Creating new text based on input data and learned patterns.\n",
        "\n",
        "By combining these three processes, Knowledge Base enables Large Language Models (LLMs) to generate more relevant responses referencing your own data from internal sources across S3, HPE Ezmeral Data Fabric, and HPE GreenLake for File Storage.\n",
        "The result is tailored, context-aware responses for your specific use cases.\n",
        "\n",
        "### Features and Functionality\n",
        "Knowledge Base simplifies the creation and management of RAG solutions through a simple interface where you can:\n",
        "- **Securely connect enterprise data sources** to RAG solutions to generate accurate, contextually aware responses for reduced risk of inaccuracies or hallucinations.\n",
        "- **Manage the RAG workflow**, including data ingestion, retrieval, and automation.\n",
        "- **Utilize advanced options** such as custom prompts and endpoint APIs for enhanced control and flexibility.\n",
        "- **Generate access tokens** for secure user and client application access to RAG coordinator endpoints.\n",
        "- **Experiment** with the built-in playground and session context management features.\n",
        "\n",
        "## Knowledge Base Architecture\n",
        "\n",
        "The following diagram shows the key components and workflow in HPE AI Essentials Software Knowledge Base:\n",
        "\n",
        "![Knowledge Base Architecture](docs/Knowledge-base-architecture.png)\n",
        "\n",
        "The following list describes the Knowledge Base components included in HPE AI Essentials Software:\n",
        "\n",
        "1. **Private Data**: Users can connect private data sources or upload local files. The system processes these files into structured chunks for efficient indexing and retrieval.\n",
        "\n",
        "2. **Embedding**: HPE AI Essentials Software integrates the GPU-Optimized model (e.g., NVIDIA Retrieval QA E5 Embedding v5) to perform semantic search. It converts text into high-dimensional numerical embeddings to capture meaning.\n",
        "\n",
        "3. **Vector DB**: The data source is stored as vectors in a vector database (Weaviate). Each Knowledge Base corresponds to one Collection in the database.\n",
        "\n",
        "4. **Input**: A user’s query (question, statement, or prompt).\n",
        "\n",
        "5. **Search and Retrieval**: When a user submits a query, the system scans the Weaviate vector database to fetch the data that answers the user’s query.\n",
        "\n",
        "6. **Contextual Data Prompt**: The system fetches the most relevant context to the user’s query which is then used to create a detailed, context-aware prompt for the LLM.\n",
        "\n",
        "7. **LLM**: NVIDIA AI Enterprise provides prebuilt containers for large language models (LLMs) used to generate responses. These are GPU-optimized and scalable.\n",
        "\n",
        "8. **Output**: The final response generated by the LLM after processing the contextual data prompt.\n",
        "\n",
        "### Accessing and Managing\n",
        "You can manage Knowledge Base configurations, statuses, and API tokens through the HPE AI Essentials Software UI. This lab focuses on the API interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import warnings\n",
        "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
        "\n",
        "# Suppress insecure request warnings for lab environment\n",
        "warnings.simplefilter('ignore', InsecureRequestWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "\n",
        "In this section, we configure the connection to the RAG endpoint. \n",
        "\n",
        "**NOTE**: Please update the `AUTH_TOKEN` and `APP_NAME` with the values provided in your specific lab environment or Knowledge Base settings.\n",
        "\n",
        "- **RAG_ENDPOINT**: The URL of the RAG Coordinator.\n",
        "- **AUTH_TOKEN**: Your security token to authenticate with the service.\n",
        "- **APP_NAME**: The specific Collection ID or Application Name for your Knowledge Base.\n",
        "- **ENABLE_CITATIONS**: Toggle this to \"true\" if you want the response to include source citations.\n",
        "- **SYSTEM_CONTEXT**: The prompt template used to guide the LLM's behavior and inject context.\n",
        "\n",
        "### **How to Generate API Tokens**\n",
        "\n",
        "To run this lab, you need APP name and tokens for the Knowledge Base. Follow these steps:\n",
        "\n",
        "**1. Access Gen AI Studio**\n",
        "Open the Generative AI Studio from your dashboard.\n",
        "![Access Gen AI](docs/accessing_Gen%20AI.png)\n",
        "\n",
        "**2. Navigate to Knowledge Base**\n",
        "Click on the \"Knowledge Base\" option in the side menu.\n",
        "![Model Endpoints](docs/kb-option.png)\n",
        "\n",
        "**3. Select a knowledge base**\n",
        "Locate and select the knowledge base you need (e.g., `llama3-8b 1`).\n",
        "![Select Model](docs/select-kb.png)\n",
        "\n",
        "**4. Copy the Metadata name and RAG endpoint**\n",
        "On this screen, copy the value shown under Metadata name and paste it into the `APP_NAME` variable in your code.\n",
        "Next, copy the RAG Coordinator Endpoint URL and paste it into the `RAG_ENDPOINT` variable, replacing the placeholder value.\n",
        "\n",
        "![Copy Endpoint](docs/metadata.png)\n",
        "\n",
        "**5. Generate Token**\n",
        "Click the **\"Generate Token\"** button.\n",
        "![Generate Button](docs/Generate_token.png)\n",
        "\n",
        "**6. Copy the API Key**\n",
        "A token will be generated. Copy this key immediately and paste it into the `AUTH_TOKEN` variable below.\n",
        "![Copy Key](docs/copy_key.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Endpoint Configuration\n",
        "RAG_ENDPOINT = \"YOUR_RAG_ENDPOINT_HERE\"\n",
        "RAG_API_PATH = \"/v1/chat/completions\"\n",
        "\n",
        "# User Credentials\n",
        "AUTH_TOKEN = \"YOUR_AUTH_TOKEN_HERE\"\n",
        "APP_NAME = \"YOUR_APP_NAME_HERE\"\n",
        "RAG_MODEL = \"meta/llama3-8b-instruct\"\n",
        "\n",
        "# Toggle Citations\n",
        "ENABLE_CITATIONS = \"false\" # Set to \"true\" to enable citations\n",
        "\n",
        "# Output Language\n",
        "output_language = \"English\"\n",
        "\n",
        "# System Context for RAG\n",
        "# This template defines how the LLM should behave and where context/history is injected.\n",
        "SYSTEM_CONTEXT = \"\"\"- You are an AI assistant who analyzes the input query and provides responses strictly based on the retrieved context and previous chat conversations.\n",
        "- If no relevant context or prior chat conversations is available, do not respond and instead state: \"I don't have enough information to answer the question.\"\n",
        "- You must not rely on general knowledge or any data from your training. Use only the specific details from the given context and prior chat conversations.\n",
        "- If no relevant context is retrieved, do not attempt to generate a response.\n",
        "- Your answers must remain strictly within the boundaries of the available context and previous chat conversations.\n",
        "- If the user sends a greeting or a polite remark, reply briefly in a friendly manner\n",
        "- Do not include greetings in responses to non-greeting queries.\n",
        "- Don't assume or hallucinate when responding\n",
        "- Do not include the phrase 'Based on the provided context and previous chat conversation' in the responses.\n",
        "\n",
        "Context:\n",
        "{context} \n",
        "\n",
        "Chat_Conversations:\n",
        "{chat_history}\n",
        "\n",
        "Query:\n",
        "{query}\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Query Function\n",
        "\n",
        "We will define a helper function `query_rag` that:\n",
        "1. Constructs the HTTP headers with Authentication and Application Name.\n",
        "2. Builds the JSON payload with the `system` message and `user` query.\n",
        "3. Sends the POST request to the RAG Coordinator.\n",
        "4. Parses and returns the response, including extracting citations if enabled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def query_rag(text):\n",
        "    \"\"\"Query RAG endpoint with language-specific instruction.\"\"\"\n",
        "    url = f\"{RAG_ENDPOINT}{RAG_API_PATH}\"\n",
        "    \n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {AUTH_TOKEN}\",\n",
        "        \"X-SA-NAME\": APP_NAME,\n",
        "        \"X-ENABLE-CITATIONS\": ENABLE_CITATIONS\n",
        "    }\n",
        "    \n",
        "    # Payload for Chat Completion\n",
        "    payload = {\n",
        "        \"model\": RAG_MODEL,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_CONTEXT},\n",
        "            {\"role\": \"user\", \"content\": text},\n",
        "        ],\n",
        "        \"max_tokens\": 2048,\n",
        "        \"temperature\": 0,\n",
        "        \"frequency_penalty\": 0,\n",
        "        \"presence_penalty\": 1,\n",
        "        \"top_p\": 0.5\n",
        "    }\n",
        "    \n",
        "    print(f\"Querying Knowledge Base... (Citations: {ENABLE_CITATIONS})\")\n",
        "    \n",
        "    try:\n",
        "        response = requests.post(url, headers=headers, json=payload, verify=False) \n",
        "        response.raise_for_status()\n",
        "        result = response.json()\n",
        "        \n",
        "        # Handle Citations if enabled\n",
        "        if \"metadata\" in result and \"citations\" in result[\"metadata\"] and ENABLE_CITATIONS.lower() == \"true\":\n",
        "            print(\"\\n--- Citations ---\")\n",
        "            for citation in result[\"metadata\"][\"citations\"]:\n",
        "                doc_name = citation.get(\"doc_id\", \"Unknown Document\")\n",
        "                doc_url = citation.get(\"doc_url\", \"No URL provided\")\n",
        "                print(f\"Document: {doc_name} ({doc_url})\")\n",
        "                \n",
        "                if \"chunks\" in citation:\n",
        "                    for idx, chunk in enumerate(citation[\"chunks\"]):\n",
        "                        snippet = chunk.get(\"snippet\", \"\").strip()\n",
        "                        score = chunk.get(\"score\", \"N/A\")\n",
        "                        print(f\"  - Chunk {idx+1} (Score: {score}): {snippet[:150]}...\")\n",
        "            print(\"-----------------\\n\")\n",
        "        \n",
        "        # Handle Chat Completion Response\n",
        "        if \"choices\" in result and len(result[\"choices\"]) > 0:\n",
        "            answer = result[\"choices\"][0][\"message\"][\"content\"]\n",
        "            return answer\n",
        "        else:\n",
        "            return f\"Unexpected response format: {str(result)}\"\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"RAG Error: {e}\")\n",
        "        return \"I'm sorry, I couldn't connect to the knowledge base.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Interactive Lab\n",
        "\n",
        "Now, test the Knowledge Base by entering queries below. \n",
        "Ensure your `AUTH_TOKEN` and `APP_NAME` are valid before running this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"--- HPE AI Essentials Knowledge Base Lab ---\")\n",
        "print(\"Type 'exit', 'quit', or 'q' to stop the session.\\n\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Enter your query: \")\n",
        "    \n",
        "    if user_input.lower() in ['exit', 'quit', 'q']:\n",
        "        print(\"Exiting session.\")\n",
        "        break\n",
        "    \n",
        "    if not user_input.strip():\n",
        "        continue\n",
        "        \n",
        "    # Get Response\n",
        "    answer = query_rag(user_input)\n",
        "    \n",
        "    print(f\"\\nAI Response:\\n{answer}\\n\")\n",
        "    print(\"-\" * 50 + \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
